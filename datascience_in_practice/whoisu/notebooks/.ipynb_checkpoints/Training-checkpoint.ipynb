{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your first computer vision model\n",
    "### How to use data augmentation and convolutional neural networks to train a face recognition system\n",
    "\n",
    "In this tutorial you work with images taken from the participants beforehand with our WhoRU webapp.\n",
    "\n",
    "\n",
    "At the end of the tutorial you can save your model, which later can then be plugged into our WhoRU webapp to check how good it performs as compared to the other uploads.\n",
    "\n",
    "Have fun!\n",
    "\n",
    "Particular parameters you should play with are highlighted by a comment #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    shape_predictor.__init__(shape_predictor, str)\ndid not match C++ signature:\n    __init__(boost::python::api::object, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n    __init__(_object*)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d369ccbec48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mface_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_face_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/Whoisit_Boston/datascience/datascience_in_practice/whoisu/notebooks/face_detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#from keras.preprocessing.image import  array_to_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/face_recognition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0.1.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_landmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpredictor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_predictor_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpose_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcnn_face_detection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detector_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    shape_predictor.__init__(shape_predictor, str)\ndid not match C++ signature:\n    __init__(boost::python::api::object, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n    __init__(_object*)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import os\n",
    "import glob\n",
    "#from face_detection import get_face, convert_to_face_only\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "# Keras stuff\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense,Reshape\n",
    "import pickle\n",
    "from face_detection import convert_to_face_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ghvoi/Projects/Whoisit_Boston/images_RATIO_0p5/\" #where are the images located\n",
    "MODEL_FILENAME= 'dry_run_people_ratio0p5_model.h5' #where to store the Keras model (including weights)\n",
    "# Image dimensions for cropped pictures\n",
    "FINAL_HEIGHT=120\n",
    "FINAL_WIDTH=120\n",
    "AUGMENT_VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview of the available  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [ name for name in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, name)) ]\n",
    "nclasses = len(classes)\n",
    "files = {cl:glob.glob(DATA_PATH+cl+\"/*.jpg\") for cl in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl,fls in files.items():\n",
    "    print(\"Category '{}' has {} images\".format(cl,len(fls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into the training and test sets and run face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_test = {}\n",
    "files_train = {}\n",
    "for cl in classes:\n",
    "    files_train[cl], files_test[cl] = train_test_split(files[cl], test_size=1) # 2 pictures go into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train/test - directory structure and put the cropped pictures into it\n",
    "train_dest_directory = DATA_PATH[:-1]+\"_Train_Test/\"\n",
    "if not os.path.exists(train_dest_directory):\n",
    "    os.makedirs(train_dest_directory)\n",
    "\n",
    "for t_ in [\"Train\", \"Test\"]:\n",
    "    for cl,fns in files_test.items() if t_ == \"Test\" else files_train.items():\n",
    "        tmp_dir = train_dest_directory+t_+\"/\"+cl+\"/\"\n",
    "        #print(tmp_dir)\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.makedirs(tmp_dir)\n",
    "        for f in fns:\n",
    "            print(\" \",f, \"->\", tmp_dir +f[f.rindex(\"/\")+1:])\n",
    "            # skip since pics are already cropped\n",
    "            convert_to_face_only(f, tmp_dir +f[f.rindex(\"/\")+1:], final_height=FINAL_HEIGHT, final_width=FINAL_WIDTH) # runs face detection! 120x120 pictures\n",
    "            #copyfile(f,tmp_dir +f[f.rindex(\"/\")+1:] )\n",
    "            # face detection is based on default OpenCV (Haar) cascade filter\n",
    "            # see also https://realpython.com/blog/python/face-recognition-with-python/\n",
    "            #copyfile(f,tmp_dir +f[f.rindex(\"/\")+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Typical CNN](typical_cnn.png)\n",
    "Figure taken from \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=( FINAL_HEIGHT, FINAL_WIDTH,3), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.25)) #0.5 is too high\n",
    "\n",
    "model.add(Dense(nclasses))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #TODO: change this for multiclass cases\n",
    "              optimizer='adam', #better\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # should be around 8 if we have 30pics per person\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, #normalisation\n",
    "        rotation_range=5, #play around with this, was 5\n",
    "        shear_range=0.1, #was 0.1\n",
    "        zoom_range=0.1, #was 0.1\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "if AUGMENT_VALIDATION:\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                      rotation_range=5, #play around with this\n",
    "                                      shear_range=0.1,\n",
    "                                      zoom_range=0.1,)\n",
    "else:\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of  train_dest_directory+'Train/', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dest_directory+'Train/',  # this is the target directory\n",
    "        target_size=(FINAL_HEIGHT, FINAL_WIDTH),  # in case we want to resize images\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "                              \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "       train_dest_directory+'Test/',\n",
    "        target_size=(FINAL_HEIGHT, FINAL_WIDTH),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIMAGES_IN_TRAIN = np.sum([len(f) for f in files_train.values()])\n",
    "NIMAGES_IN_TEST = np.sum([len(f) for f in files_test.values()])\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=NIMAGES_IN_TRAIN // batch_size, \n",
    "        epochs=50, #was 50\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NIMAGES_IN_TEST if not AUGMENT_VALIDATION else 10*NIMAGES_IN_TEST  ) #was 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_FILENAME)  # creates a HDF5 file 'lfw_people_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to resolve Keras internal class labels (back to folder names used in the App)\n",
    "class_dictionary = train_generator.class_indices\n",
    "ids_to_classes = [k for (k,v) in class_dictionary.items()]\n",
    "with open(MODEL_FILENAME.replace(\"h5\", \"pickle\"), 'wb') as handle:\n",
    "    pickle.dump(ids_to_classes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig,ax = plt.subplots()\n",
    "# \"bo\" is for \"blue dot\"\n",
    "ax.plot(epochs, loss_values, 'bo',label=\"Training\")\n",
    "# g+ is for \"green crosses\"\n",
    "ax.plot(epochs, val_loss_values, 'g+',label=\"Validation\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, acc_values, 'bo', label=\"Training\")\n",
    "# b+ is for \"green crosses\"\n",
    "ax.plot(epochs, val_acc_values, 'g+', label=\"Validation\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"img_Train_Test/Train/Djork-Arne_Clevert/853e2a0b-e1a3-45b8-a171-71a8d7fc00f7.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
