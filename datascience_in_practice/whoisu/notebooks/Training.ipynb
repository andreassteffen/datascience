{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train your first computer vision model\n",
    "### How to use data augmentation and convolutional neural networks to train a face recognition system\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "We all know that feeling... \n",
    "\n",
    "> You arrive at a conference / workshop and you have a hard time remembering names. \n",
    "\n",
    "![Typical CNN](https://imgs.xkcd.com/comics/names.png)\n",
    "\n",
    "## Solution\n",
    "\n",
    "As Data Scientists we want a solution, hands on!\n",
    "\n",
    "We implement a face recognition app called **WhoRU** that runs on your smartphone and recognises previously tagged participants.\n",
    "\n",
    "## Step by step\n",
    "\n",
    "1. You  took pictures of all participants with the **WhoRU** app from your mobile phone and assigned the correct name to them (__data collection & labelling__)\n",
    "\n",
    "2. Here you will train a deep learning model (based on convolutional networks) to recognise and correctly label the participants (__data preparation & training__)\n",
    "\n",
    "3. You can try your model in the \"Predict\"-mode of **WhoRU** and apply it to new photos taken (__generalisation & evaluation__)\n",
    "\n",
    "4. Have fun networking...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all required packages\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "import os\n",
    "import glob\n",
    "#from face_detection import get_face, convert_to_face_only\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "# Keras stuff\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense,Reshape\n",
    "import pickle\n",
    "from face_detection import convert_to_face_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEAM_NAME=\"TeamAndreasEren\" #set this to your unique team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some important parameters\n",
    "###########################################################\n",
    "DATA_PATH = \"/home/fisc/datascience/datascience_in_practice/whoisu/notebooks/new_pics/\" #where are the images located\n",
    "#############################################################\n",
    "MODEL_FILENAME= 'model_{}.h5'.format(TEAM_NAME.replace(\" \",\"_\").strip()) #where to store the Keras model (including weights)\n",
    "# Image dimensions for cropped pictures\n",
    "FINAL_HEIGHT=120\n",
    "FINAL_WIDTH=120\n",
    "AUGMENT_VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview of the available  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [ name for name in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, name)) ]\n",
    "nclasses = len(classes)\n",
    "files = {cl:glob.glob(DATA_PATH+cl+\"/*.jpg\") for cl in classes}\n",
    "df = pd.DataFrame(data={\"Participant\": [ f.replace(\"_\", \" \") for f in list(files.keys())], \n",
    "                   \"Number of photos\": [len(f) for f in files.values()] }).set_index(\"Participant\")\n",
    "df.plot(kind=\"barh\");\n",
    "#for cl,fls in files.items():\n",
    "#    print(\"Category '{}' has {} images\".format(cl,len(fls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into the training and test sets and run face detection\n",
    "\n",
    "It is important to validate your model on data it has not used as part of the tranining process. One way to do this is to split the data into *training* and *test* sets.  The model is purely trained on the *training* data and evaluated on \"unseen\" *test* data. This checks whether the model is able to generalise and probes the model for *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_test = {}\n",
    "files_train = {}\n",
    "for cl in classes:\n",
    "    files_train[cl], files_test[cl] = train_test_split(files[cl], test_size=5) # 2 pictures go into test set\n",
    "    # create the train/test - directory structure and put the cropped pictures into it\n",
    "train_dest_directory = DATA_PATH[:-1]+\"_Train_Test/\"\n",
    "if not os.path.exists(train_dest_directory):\n",
    "    os.makedirs(train_dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run face detection on all photos in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froms = []\n",
    "tos = []\n",
    "for t_ in [\"Train\", \"Test\"]:\n",
    "    for cl,fns in files_test.items() if t_ == \"Test\" else files_train.items():\n",
    "        tmp_dir = train_dest_directory+t_+\"/\"+cl+\"/\"\n",
    "        #print(tmp_dir)\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.makedirs(tmp_dir)\n",
    "        #for f in fns:\n",
    "        for j in tqdm_notebook(range(len(fns)), desc='{}:'.format(cl.replace(\"_\", \" \"))):\n",
    "            f = fns[j]\n",
    "            froms.append(f)\n",
    "            tos.append(tmp_dir +f[f.rindex(\"/\")+1:])\n",
    "            convert_to_face_only(f, tmp_dir +f[f.rindex(\"/\")+1:], final_height=FINAL_HEIGHT, final_width=FINAL_WIDTH) # runs face detection! 120x120 pictures\n",
    "df = pd.DataFrame(data={\"Input file\": froms, \"Target file\": tos})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the deep learning model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement an \"standard\" architecture to do the face recognition, which is illustrated in the figure below\n",
    "\n",
    "![Typical CNN](typical_cnn.png)\n",
    "Figure taken from \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=( FINAL_HEIGHT, FINAL_WIDTH,3), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.25)) #Dropout, acts as regularisation --> Fights overfitting\n",
    "\n",
    "model.add(Dense(nclasses))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #since we do multiclass classification\n",
    "              optimizer='adam', #Adam Optimiser\n",
    "              metrics=['accuracy']) # we use accuracy as a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of data augmentation using Keras\n",
    "\n",
    "### Try an experiment with the various parameters to get an intuition for their action\n",
    "\n",
    "### Explanations\n",
    "\n",
    "- **DATA_PATH**: Directory that contains images.\n",
    "- **NUM_EXAMPLES**: Number of persons (classes) to use.\n",
    "- **NUM_MUTATIONS_PER_PIC**: Number of random transformations per person.\n",
    "- **ROTATION_RANGE**:  Integer degree range for random rotations\n",
    "- **WIDTH_SHIFT_RANGE**: Float (fraction of total width). Range for random horizontal shifts.\n",
    "- **HEIGHT_SHIFT_RANGE**: Float (fraction of total height). Range for random vertical shifts.\n",
    "- **SHEAR_RANGE**: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "- **ZOOM_RANGE**:  Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\n",
    "- **HORIZONTAL_FLIP**: Whether or not to flip (mirror) the images w.r.t. to a vertical axis (Randomly flip inputs horizontally).\n",
    "- **SEED**: Seed for the utilised pseudo-random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_random_pic_matrix(DATA_PATH, NUM_EXAMPLES, NUM_MUTATIONS_PER_PIC,\n",
    "                           ROTATION_RANGE=40, \n",
    "                           WIDTH_SHIFT_RANGE=0.2,\n",
    "                           HEIGHT_SHIFT_RANGE=0.2,\n",
    "                           SHEAR_RANGE=0.2,\n",
    "                           ZOOM_RANGE=0.2,\n",
    "                           HORIZONTAL_FLIP=True,\n",
    "                           seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    classes = [ name for name in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, name)) ]\n",
    "    nclasses = len(classes)\n",
    "    files = {cl:glob.glob(DATA_PATH+cl+\"/*.jpg\") for cl in classes}\n",
    "    np.random.shuffle(classes)\n",
    "    random_choices = [files[c][np.random.randint(0,len(files[c]),size=1)[0]] for c in classes[:NUM_EXAMPLES]]  \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "    rotation_range=ROTATION_RANGE,\n",
    "    width_shift_range=WIDTH_SHIFT_RANGE,\n",
    "    height_shift_range=HEIGHT_SHIFT_RANGE,\n",
    "    shear_range=SHEAR_RANGE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    horizontal_flip=HORIZONTAL_FLIP,\n",
    "    fill_mode='nearest')\n",
    "    fig, axs = plt.subplots(nrows=NUM_EXAMPLES, ncols=1+NUM_MUTATIONS_PER_PIC, figsize=(8,8))\n",
    "    for i,f in enumerate(random_choices):\n",
    "        axrow = axs[i]\n",
    "        img = load_img(f)  # this is a PIL image\n",
    "        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "        ax = axrow[0]\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        j=0\n",
    "        for batch in datagen.flow(x, batch_size=1):\n",
    "            ax = axrow[1+j]\n",
    "            ax.imshow(image.array_to_img(batch[0]))\n",
    "            j+=1\n",
    "            ax.axis(\"off\")\n",
    "            if j==NUM_MUTATIONS_PER_PIC:\n",
    "                break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_pic_matrix(DATA_PATH=train_dest_directory+\"Train/\", \n",
    "                       NUM_EXAMPLES=2, \n",
    "                       NUM_MUTATIONS_PER_PIC=4,\n",
    "                       ROTATION_RANGE=40, \n",
    "                       WIDTH_SHIFT_RANGE=0.2,\n",
    "                       HEIGHT_SHIFT_RANGE=0.2,\n",
    "                       SHEAR_RANGE=0.2,\n",
    "                       ZOOM_RANGE=0.2,\n",
    "                       HORIZONTAL_FLIP=True,\n",
    "                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_pic_matrix(DATA_PATH=train_dest_directory+\"Train/\", \n",
    "                       NUM_EXAMPLES=2,  \n",
    "                       NUM_MUTATIONS_PER_PIC=5,\n",
    "                       ROTATION_RANGE=5, \n",
    "                       WIDTH_SHIFT_RANGE=0.1,\n",
    "                       HEIGHT_SHIFT_RANGE=0.1,\n",
    "                       SHEAR_RANGE=0.1,\n",
    "                       ZOOM_RANGE=0.1,\n",
    "                       HORIZONTAL_FLIP=True,\n",
    "                       seed=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline \n",
    "\n",
    "Back to our original goal...\n",
    "\n",
    "As part of training we enrich (augment) the data by transformations. This is done to fight overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # should be around 8 if we have 30pics per person\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, #normalisation\n",
    "        rotation_range=5, #play around with this, was 5\n",
    "        shear_range=0.1, #was 0.1\n",
    "        zoom_range=0.1, #was 0.1\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "if AUGMENT_VALIDATION:\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                      rotation_range=5, #play around with this\n",
    "                                      shear_range=0.1,\n",
    "                                      zoom_range=0.1,)\n",
    "else:\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of  train_dest_directory+'Train/', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dest_directory+'Train/',  # this is the target directory\n",
    "        target_size=(FINAL_HEIGHT, FINAL_WIDTH),  # in case we want to resize images\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "                              \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "       train_dest_directory+'Test/',\n",
    "        target_size=(FINAL_HEIGHT, FINAL_WIDTH),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the fitting (learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIMAGES_IN_TRAIN = np.sum([len(f) for f in files_train.values()])\n",
    "NIMAGES_IN_TEST = np.sum([len(f) for f in files_test.values()])\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=50, #since we use data augmentation all will be distinct \n",
    "        epochs=20, #was 50\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NIMAGES_IN_TEST if not AUGMENT_VALIDATION else 10*NIMAGES_IN_TEST  ) #was 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(MODEL_FILENAME)  \n",
    "# Allows us to resolve Keras internal class labels (back to folder names used in the App)\n",
    "class_dictionary = train_generator.class_indices\n",
    "ids_to_classes = [k for (k,v) in class_dictionary.items()]\n",
    "with open(MODEL_FILENAME.replace(\"h5\", \"pickle\"), 'wb') as handle:\n",
    "    pickle.dump(ids_to_classes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Inspect the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs epochs on training and test set (the smaller the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig,ax = plt.subplots()\n",
    "# \"bo\" is for \"blue dot\"\n",
    "ax.plot(epochs, loss_values, 'bo',label=\"Training\")\n",
    "# g+ is for \"green crosses\"\n",
    "ax.plot(epochs, val_loss_values, 'g+',label=\"Validation\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs epochs on training and test set (the higher the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, acc_values, 'bo', label=\"Training\")\n",
    "# b+ is for \"green crosses\"\n",
    "ax.plot(epochs, val_acc_values, 'g+', label=\"Validation\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
